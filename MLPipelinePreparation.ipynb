{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "# import SQL-libraries\n",
    "from sqlalchemy.engine import create_engine\n",
    "# import nltk libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# import sk-learn libraries\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom randomstate\n",
    "myseed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download NLTK package\n",
    "nltk.download(['stopwords', 'wordnet', 'punkt', 'averaged_perceptron_tagger'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///DisasterResponse.db')\n",
    "df = pd.read_sql_table('DisasterResponse', con=engine)\n",
    "# Create X and Y from the df\n",
    "X = df.iloc[:, :4]\n",
    "Y = df.iloc[:, 4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''\n",
    "    tokenizes given text string.\n",
    "    arg:\n",
    "        text: string. twitter message\n",
    "    return:\n",
    "        cleaned: list. tokens of message.\n",
    "    '''\n",
    "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    # get list of all urls using regex\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    # replace each url in text string with placeholder\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, 'urlsub')\n",
    "    # initiate Stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    # initiate Lemmatizer\n",
    "    lemmatizer =  WordNetLemmatizer()\n",
    "    # removing special characters and tranforming capital letters\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()).split()\n",
    "    # removing stop words\n",
    "    text_tokens = [word for word in text if word not in stopwords.words('english')]\n",
    "    # lemmatize tokens\n",
    "    cleaned = [lemmatizer.lemmatize(word) for word in text_tokens]\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up basic pipline\n",
    "pipeline = Pipeline([\n",
    "    ('vec', TfidfVectorizer(tokenizer = tokenize)),\n",
    "    ('classifier', MultiOutputClassifier(RandomForestClassifier()))\n",
    "     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into training and test dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state = myseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting pipeline\n",
    "test = pipeline.fit(x_train[\"message\"], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting pipeline with test dataset\n",
    "y_pred = pipeline.predict(x_test[\"message\"])\n",
    "# transforming into pd.Dataframe\n",
    "y_pred = pd.DataFrame(y_pred, columns=y_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(y_test, y_pred):\n",
    "    '''\n",
    "    Function: \n",
    "        gets Precision, Recall and F1 scores of the model for each output category of the trainingsset and modelaverages\n",
    "    Args:\n",
    "        y_test (pd.Dataframe): labels of the testdata.\n",
    "        y_pred (pd.Dataframe): predicted outcome of the testdata\n",
    "    Return:\n",
    "        df_scores (pd.Dataframe): Precision, Recall and F1 scores of categories\n",
    "        averages (pd.Series): model-averages for Precision, Recall and F1.\n",
    "    '''\n",
    "    scores = []\n",
    "    # Extract Precision, Recall and F1 scores for each output category\n",
    "    for column in y_test.columns:\n",
    "        scores.append(classification_report(y_test[column], y_pred[column]).split()[-4:-1])\n",
    "    df_scores = pd.DataFrame(scores, index=y_test.columns, columns = ['Precision', 'Recall', 'F1-Score'])\n",
    "    df_scores = df_scores.astype(float)\n",
    "    # calculate averages over all output categories\n",
    "    averages = pd.Series([df_scores[column].mean() for column in df_scores.columns], index = df_scores.columns)\n",
    "    return df_scores, averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "scores, means = get_scores(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    '''\n",
    "    Class: allows fitting, predicting and scoring classifier with given parameters.\n",
    "    '''\n",
    "    def __init__(self, classifier, parameters):\n",
    "        '''\n",
    "        Function:\n",
    "            Instanciates Pipeline object with given parameters.\n",
    "        Args:\n",
    "            classifier (Estimator): classifier object to use as estimator.\n",
    "            parameters (Dictionary): hyperparameter to use in gridsearch.\n",
    "        '''\n",
    "        self.classifier = classifier\n",
    "        self.parameters = parameters\n",
    "        # Instanciating Pipeline object with customer tokenizer\n",
    "        self.pipeline = Pipeline([\n",
    "            ('vec', TfidfVectorizer(tokenizer = tokenize)),\n",
    "            ('classifier', MultiOutputClassifier(self.classifier))\n",
    "                                ])\n",
    "    def fit(self, X, y, verbose=3, scoring='f1_micro'):\n",
    "        '''\n",
    "        Function:\n",
    "            Runs Gridsearch on classifier with given hyperparameter\n",
    "        Args:\n",
    "            X (pd.Dataframe): Predictor of trainingsdata\n",
    "            y (pd.Dataframe): labels of trainingsdata\n",
    "        Return:\n",
    "            cv (Estimator): fitted best estimator\n",
    "        '''\n",
    "        self.cv = GridSearchCV(self.pipeline, param_grid=self.parameters, verbose=verbose, scoring=scoring)\n",
    "        self.columns = y.columns\n",
    "        self.cv.fit(X, y)\n",
    "        self.best_params_ = self.cv.best_params_\n",
    "        self.best_estimator_ = self.cv.best_estimator_\n",
    "        self.best_score_ = self.cv.best_score_\n",
    "        return self.cv\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Function:\n",
    "            Predicts labels on the given dataset\n",
    "        Args:\n",
    "            X (pd.Dataframe): Predictor of dataset\n",
    "        Return:\n",
    "            y_pred: predicted labels of the dataset\n",
    "        '''\n",
    "        self.y_pred = self.cv.predict(X)\n",
    "        self.y_pred = pd.DataFrame(self.y_pred, columns=self.columns)\n",
    "        return self.y_pred\n",
    "\n",
    "    def get_scores(self, y_test):\n",
    "        '''\n",
    "        Function: \n",
    "            gets Precision, Recall and F1 scores of the model for each output category of the trainingsset and modelaverages.\n",
    "        Args:\n",
    "            y_test (pd.Dataframe): labels of the testdata.\n",
    "            y_pred (pd.Dataframe): predicted outcome of the testdata.\n",
    "        Return:\n",
    "            df_scores (pd.Dataframe): Precision, Recall and F1 scores of categories.\n",
    "            averages (pd.Series): model-averages for Precision, Recall and F1.\n",
    "        '''\n",
    "        scores = []\n",
    "        for column in y_test.columns:\n",
    "            scores.append(classification_report(y_test[column], self.y_pred[column]).split()[-4:-1])\n",
    "        df_scores = pd.DataFrame(scores, index=y_test.columns, columns = ['Precision', 'Recall', 'F1-Score'])\n",
    "        df_scores = df_scores.astype(float)\n",
    "        averages = pd.Series([df_scores[column].mean() for column in df_scores.columns], index = df_scores.columns)\n",
    "        return df_scores, averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[CV] classifier__estimator__min_samples_split=2, classifier__estimator__n_estimators=100 \n",
      "[CV]  classifier__estimator__min_samples_split=2, classifier__estimator__n_estimators=100, score=0.6395359057748088, total= 6.9min\n",
      "[CV] classifier__estimator__min_samples_split=2, classifier__estimator__n_estimators=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  8.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  classifier__estimator__min_samples_split=2, classifier__estimator__n_estimators=100, score=0.6447088946434286, total= 7.1min\n",
      "[CV] classifier__estimator__min_samples_split=2, classifier__estimator__n_estimators=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 17.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  classifier__estimator__min_samples_split=2, classifier__estimator__n_estimators=100, score=0.6410732466910224, total= 7.1min\n",
      "[CV] classifier__estimator__min_samples_split=2, classifier__estimator__n_estimators=200 \n",
      "[CV]  classifier__estimator__min_samples_split=2, classifier__estimator__n_estimators=200, score=0.6449636825063446, total=12.3min\n",
      "[CV] classifier__estimator__min_samples_split=2, classifier__estimator__n_estimators=200 \n",
      "[CV]  classifier__estimator__min_samples_split=2, classifier__estimator__n_estimators=200, score=0.6442828047716032, total=12.3min\n",
      "[CV] classifier__estimator__min_samples_split=2, classifier__estimator__n_estimators=200 \n",
      "[CV]  classifier__estimator__min_samples_split=2, classifier__estimator__n_estimators=200, score=0.6395826462387192, total=12.4min\n",
      "[CV] classifier__estimator__min_samples_split=4, classifier__estimator__n_estimators=100 \n",
      "[CV]  classifier__estimator__min_samples_split=4, classifier__estimator__n_estimators=100, score=0.646574229854383, total= 5.9min\n",
      "[CV] classifier__estimator__min_samples_split=4, classifier__estimator__n_estimators=100 \n",
      "[CV]  classifier__estimator__min_samples_split=4, classifier__estimator__n_estimators=100, score=0.6486829577911775, total= 6.0min\n",
      "[CV] classifier__estimator__min_samples_split=4, classifier__estimator__n_estimators=100 \n",
      "[CV]  classifier__estimator__min_samples_split=4, classifier__estimator__n_estimators=100, score=0.6444866920152091, total= 6.1min\n",
      "[CV] classifier__estimator__min_samples_split=4, classifier__estimator__n_estimators=200 \n",
      "[CV]  classifier__estimator__min_samples_split=4, classifier__estimator__n_estimators=200, score=0.6466680205401956, total=10.4min\n",
      "[CV] classifier__estimator__min_samples_split=4, classifier__estimator__n_estimators=200 \n",
      "[CV]  classifier__estimator__min_samples_split=4, classifier__estimator__n_estimators=200, score=0.6479043990301351, total=10.5min\n",
      "[CV] classifier__estimator__min_samples_split=4, classifier__estimator__n_estimators=200 \n",
      "[CV]  classifier__estimator__min_samples_split=4, classifier__estimator__n_estimators=200, score=0.6453588630552417, total=10.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed: 128.9min finished\n"
     ]
    }
   ],
   "source": [
    "# Fitting and predicting RandomForestClassifier with hyperparameters\n",
    "parameters_rfc =    {\n",
    "        'classifier__estimator__n_estimators': [100, 200],\n",
    "        'classifier__estimator__min_samples_split': [2, 4]\n",
    "                }\n",
    "\n",
    "model_rfc = Model(RandomForestClassifier(), parameters=parameters_rfc)\n",
    "model_rfc.fit(x_train[\"message\"], y_train)\n",
    "y_pred = model_rfc.predict(x_test['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring RandomForestClassifier\n",
    "scores_rdf, averages_rfc = model_rfc.get_scores(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Class: Transformer that checks for every text entry in a Series if a sentence starts with a verb\n",
    "    '''\n",
    "    def starting_verb(self, text):\n",
    "        '''\n",
    "        Function: \n",
    "            checks for a text if one sentence starts with a verb\n",
    "        Arg:\n",
    "            text (string): text containing at least one sentence\n",
    "        Return:\n",
    "            Bool (bool): Whether one sentence starts with verb\n",
    "        '''\n",
    "        # splits text into lists of sentences\n",
    "        sentence_list = nltk.sent_tokenize(text)\n",
    "        for sentence in sentence_list:\n",
    "            pos_tags = nltk.pos_tag(tokenize(sentence))\n",
    "        # Check if sentence starts with verb\n",
    "            if pos_tags:\n",
    "                first_word, first_tag = pos_tags[0]\n",
    "                if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        '''\n",
    "        Function: \n",
    "            returns itself.\n",
    "        Arg:\n",
    "            X (pd.Series): Series of text-messages\n",
    "        Return:\n",
    "            self (StartingVerbExtractor object): returns reference to object\n",
    "        '''\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        '''\n",
    "        Function: \n",
    "            Applies starting_verb function on a Series.\n",
    "        Arg:\n",
    "            X (pd.Series): Series of text-messages.\n",
    "        Return:\n",
    "            X_tagged (pd.Dataframe): Booleans stating wheter a sentence in datapoint starts with verb.\n",
    "        '''\n",
    "        # Applies starting_verb function on series\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
    "        return pd.DataFrame(X_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Improved_Model:\n",
    "    '''\n",
    "    Class: allows fitting, predicting and scoring classifier with given parameters.\n",
    "    '''\n",
    "    def __init__(self, classifier, parameters):\n",
    "        '''\n",
    "        Function:\n",
    "            Instanciates Pipeline object with given parameters, custom tokenizer and StartingVerbExtractor.\n",
    "        Args:\n",
    "            classifier (Estimator): classifier object to use as estimator.\n",
    "            parameters (Dictionary): hyperparameter to use in gridsearch.\n",
    "        '''\n",
    "        self.classifier = classifier\n",
    "        self.parameters = parameters\n",
    "        # Instanciating Pipeline object with customer tokenizer\n",
    "        self.pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('vec', TfidfVectorizer(tokenizer = tokenize)),\n",
    "        ('starting_verb', StartingVerbExtractor())\n",
    "    ])),\n",
    "    ('classifier', MultiOutputClassifier(self.classifier))\n",
    "                        ])\n",
    "    def fit(self, X, y, verbose=3, scoring='f1_micro'):\n",
    "        '''\n",
    "        Function:\n",
    "            Runs Gridsearch on classifier with given hyperparameter\n",
    "        Args:\n",
    "            X (pd.Dataframe): Predictor of trainingsdata\n",
    "            y (pd.Dataframe): labels of trainingsdata\n",
    "        Return:\n",
    "            cv (Estimator): fitted best estimator\n",
    "        '''\n",
    "        self.cv = GridSearchCV(self.pipeline, param_grid=self.parameters, verbose=verbose, scoring=scoring, cv=2)\n",
    "        self.columns = y.columns\n",
    "        self.cv.fit(X, y)\n",
    "        self.best_params_ = self.cv.best_params_\n",
    "        self.best_estimator_ = self.cv.best_estimator_\n",
    "        self.best_score_ = self.cv.best_score_\n",
    "        return self.cv\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Function:\n",
    "            Predicts labels on the given dataset\n",
    "        Args:\n",
    "            X (pd.Dataframe): Predictor of dataset\n",
    "        Return:\n",
    "            y_pred: predicted labels of the dataset\n",
    "        '''\n",
    "        self.y_pred = self.cv.predict(X)\n",
    "        self.y_pred = pd.DataFrame(self.y_pred, columns=self.columns)\n",
    "        return self.y_pred\n",
    "\n",
    "    def get_scores(self, y_test):\n",
    "        '''\n",
    "        Function: \n",
    "            gets Precision, Recall and F1 scores of the model for each output category of the trainingsset and modelaverages.\n",
    "        Args:\n",
    "            y_test (pd.Dataframe): labels of the testdata.\n",
    "            y_pred (pd.Dataframe): predicted outcome of the testdata.\n",
    "        Return:\n",
    "            df_scores (pd.Dataframe): Precision, Recall and F1 scores of categories.\n",
    "            averages (pd.Series): model-averages for Precision, Recall and F1.\n",
    "        '''\n",
    "        scores = []\n",
    "        for column in y_test.columns:\n",
    "            scores.append(classification_report(y_test[column], self.y_pred[column]).split()[-4:-1])\n",
    "        df_scores = pd.DataFrame(scores, index=y_test.columns, columns = ['Precision', 'Recall', 'F1-Score'])\n",
    "        df_scores = df_scores.astype(float)\n",
    "        averages = pd.Series([df_scores[column].mean() for column in df_scores.columns], index = df_scores.columns)\n",
    "        return df_scores, averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV] classifier__estimator__C=0.1 ....................................\n",
      "[CV]  classifier__estimator__C=0.1, score=0.3643571558465176, total= 5.4min\n",
      "[CV] classifier__estimator__C=0.1 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  8.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  classifier__estimator__C=0.1, score=0.36374230671182484, total= 5.3min\n",
      "[CV] classifier__estimator__C=0.1 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 17.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  classifier__estimator__C=0.1, score=0.36827596483385483, total= 5.3min\n",
      "[CV] classifier__estimator__C=0.5 ....................................\n",
      "[CV]  classifier__estimator__C=0.5, score=0.3643571558465176, total= 5.3min\n",
      "[CV] classifier__estimator__C=0.5 ....................................\n",
      "[CV]  classifier__estimator__C=0.5, score=0.36374230671182484, total= 5.3min\n",
      "[CV] classifier__estimator__C=0.5 ....................................\n",
      "[CV]  classifier__estimator__C=0.5, score=0.36827596483385483, total= 5.4min\n",
      "[CV] classifier__estimator__C=1 ......................................\n",
      "[CV]  classifier__estimator__C=1, score=0.3643571558465176, total= 5.4min\n",
      "[CV] classifier__estimator__C=1 ......................................\n",
      "[CV]  classifier__estimator__C=1, score=0.36374230671182484, total= 5.3min\n",
      "[CV] classifier__estimator__C=1 ......................................\n",
      "[CV]  classifier__estimator__C=1, score=0.36827596483385483, total= 5.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 78.6min finished\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'y_pred'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8ef83e133dd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_svc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters_svc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel_svc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mscores_svc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverages_svc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_svc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-fa4a4a40e23d>\u001b[0m in \u001b[0;36mget_scores\u001b[0;34m(self, y_test)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mdf_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Precision'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Recall'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'F1-Score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mdf_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'y_pred'"
     ]
    }
   ],
   "source": [
    "# Fitting and predicting SupportVectorClassifier with hyperparameters\n",
    "parameters_svc = {\n",
    "        'classifier__estimator__C': [0.1,0.5,1]\n",
    "                }\n",
    "\n",
    "model_svc = Model(SVC(), parameters=parameters_svc)\n",
    "model_svc.fit(x_train['message'], y_train)\n",
    "model_svc.predict(x_test['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Scoring SupportVectorClassifier\n",
    "scores_svc, averages_svc = model_svc.get_scores(y_test)\n",
    "print(scores_svc, averages_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Precision  Recall  F1-Score\n",
      "related                      0.57    0.75      0.65\n",
      "request                      0.68    0.83      0.75\n",
      "offer                        0.99    1.00      0.99\n",
      "aid_related                  0.34    0.58      0.43\n",
      "medical_help                 0.85    0.92      0.88\n",
      "medical_products             0.91    0.95      0.93\n",
      "search_and_rescue            0.94    0.97      0.96\n",
      "security                     0.96    0.98      0.97\n",
      "military                     0.93    0.96      0.95\n",
      "water                        0.88    0.94      0.90\n",
      "food                         0.79    0.89      0.84\n",
      "shelter                      0.83    0.91      0.87\n",
      "clothing                     0.97    0.99      0.98\n",
      "money                        0.96    0.98      0.97\n",
      "missing_people               0.98    0.99      0.98\n",
      "refugees                     0.93    0.96      0.94\n",
      "death                        0.91    0.95      0.93\n",
      "other_aid                    0.76    0.87      0.81\n",
      "infrastructure_related       0.86    0.93      0.89\n",
      "transport                    0.91    0.95      0.93\n",
      "buildings                    0.90    0.95      0.92\n",
      "electricity                  0.96    0.98      0.97\n",
      "tools                        0.99    0.99      0.99\n",
      "hospitals                    0.98    0.99      0.98\n",
      "shops                        0.99    0.99      0.99\n",
      "aid_centers                  0.97    0.99      0.98\n",
      "other_infrastructure         0.91    0.95      0.93\n",
      "weather_related              0.54    0.73      0.62\n",
      "floods                       0.85    0.92      0.89\n",
      "storm                        0.82    0.91      0.86\n",
      "fire                         0.98    0.99      0.98\n",
      "earthquake                   0.83    0.91      0.87\n",
      "cold                         0.96    0.98      0.97\n",
      "other_weather                0.91    0.95      0.93\n",
      "direct_report                0.64    0.80      0.72 Precision    0.862286\n",
      "Recall       0.923714\n",
      "F1-Score     0.890000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "[CV] classifier__estimator__n_neighbors=2, classifier__estimator__weights=uniform \n",
      "[CV]  classifier__estimator__n_neighbors=2, classifier__estimator__weights=uniform, score=0.08599430920012646, total= 2.8min\n",
      "[CV] classifier__estimator__n_neighbors=2, classifier__estimator__weights=uniform \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  6.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  classifier__estimator__n_neighbors=2, classifier__estimator__weights=uniform, score=0.08935920047031158, total= 2.8min\n",
      "[CV] classifier__estimator__n_neighbors=2, classifier__estimator__weights=uniform \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 13.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  classifier__estimator__n_neighbors=2, classifier__estimator__weights=uniform, score=0.08917494519845157, total= 2.8min\n",
      "[CV] classifier__estimator__n_neighbors=2, classifier__estimator__weights=distance \n",
      "[CV]  classifier__estimator__n_neighbors=2, classifier__estimator__weights=distance, score=0.1881324374645835, total= 2.7min\n",
      "[CV] classifier__estimator__n_neighbors=2, classifier__estimator__weights=distance \n",
      "[CV]  classifier__estimator__n_neighbors=2, classifier__estimator__weights=distance, score=0.19105707476673264, total= 2.7min\n",
      "[CV] classifier__estimator__n_neighbors=2, classifier__estimator__weights=distance \n",
      "[CV]  classifier__estimator__n_neighbors=2, classifier__estimator__weights=distance, score=0.1929228474351537, total= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 38.5min finished\n"
     ]
    }
   ],
   "source": [
    "# Fitting and predicting KNeighborsClassifier with hyperparameters\n",
    "parameters_knn = {\n",
    "        'classifier__estimator__n_neighbors': [2],\n",
    "        'classifier__estimator__weights': [\"uniform\", \"distance\"]\n",
    "                }\n",
    "\n",
    "model_knn = Model(KNN(), parameters=parameters_knn)\n",
    "model_knn.fit(x_train['message'], y_train)\n",
    "model_knn.predict(x_test['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Precision  Recall  F1-Score\n",
      "related                      0.68    0.35      0.32\n",
      "request                      0.83    0.85      0.82\n",
      "offer                        0.99    0.99      0.99\n",
      "aid_related                  0.64    0.61      0.53\n",
      "medical_help                 0.86    0.91      0.88\n",
      "medical_products             0.92    0.95      0.93\n",
      "search_and_rescue            0.95    0.97      0.96\n",
      "security                     0.96    0.98      0.97\n",
      "military                     0.94    0.96      0.95\n",
      "water                        0.91    0.93      0.91\n",
      "food                         0.87    0.90      0.87\n",
      "shelter                      0.89    0.91      0.89\n",
      "clothing                     0.98    0.99      0.98\n",
      "money                        0.96    0.98      0.97\n",
      "missing_people               0.98    0.99      0.98\n",
      "refugees                     0.93    0.96      0.94\n",
      "death                        0.94    0.95      0.94\n",
      "other_aid                    0.81    0.86      0.82\n",
      "infrastructure_related       0.87    0.92      0.89\n",
      "transport                    0.92    0.95      0.93\n",
      "buildings                    0.92    0.95      0.92\n",
      "electricity                  0.97    0.98      0.97\n",
      "tools                        0.99    0.99      0.99\n",
      "hospitals                    0.98    0.99      0.98\n",
      "shops                        0.99    0.99      0.99\n",
      "aid_centers                  0.97    0.99      0.98\n",
      "other_infrastructure         0.91    0.95      0.93\n",
      "weather_related              0.73    0.75      0.68\n",
      "floods                       0.88    0.92      0.89\n",
      "storm                        0.88    0.91      0.87\n",
      "fire                         0.98    0.99      0.98\n",
      "earthquake                   0.91    0.92      0.91\n",
      "cold                         0.97    0.98      0.97\n",
      "other_weather                0.92    0.95      0.93\n",
      "direct_report                0.79    0.82      0.77 Precision    0.903429\n",
      "Recall       0.914000\n",
      "F1-Score     0.892286\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# scoring KNeighborsClassifier\n",
    "scores_knn, averages_knn = model_knn.get_scores(y_test)\n",
    "print(scores_knn, averages_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
      "[CV] classifier__estimator__min_samples_split=2, classifier__estimator__n_estimators=100 \n",
      "[CV]  classifier__estimator__min_samples_split=2, classifier__estimator__n_estimators=100, score=0.635406661047184, total= 6.4min\n",
      "[CV] classifier__estimator__min_samples_split=2, classifier__estimator__n_estimators=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  8.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  classifier__estimator__min_samples_split=2, classifier__estimator__n_estimators=100, score=0.6367458108828232, total= 6.5min\n",
      "[CV] classifier__estimator__min_samples_split=2, classifier__estimator__n_estimators=200 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 16.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  classifier__estimator__min_samples_split=2, classifier__estimator__n_estimators=200, score=0.6379340771387675, total= 9.6min\n",
      "[CV] classifier__estimator__min_samples_split=2, classifier__estimator__n_estimators=200 \n",
      "[CV]  classifier__estimator__min_samples_split=2, classifier__estimator__n_estimators=200, score=0.6381218733997714, total= 9.7min\n",
      "[CV] classifier__estimator__min_samples_split=4, classifier__estimator__n_estimators=100 \n",
      "[CV]  classifier__estimator__min_samples_split=4, classifier__estimator__n_estimators=100, score=0.6394350811485642, total= 5.8min\n",
      "[CV] classifier__estimator__min_samples_split=4, classifier__estimator__n_estimators=100 \n",
      "[CV]  classifier__estimator__min_samples_split=4, classifier__estimator__n_estimators=100, score=0.6400329005346337, total= 5.8min\n",
      "[CV] classifier__estimator__min_samples_split=4, classifier__estimator__n_estimators=200 \n",
      "[CV]  classifier__estimator__min_samples_split=4, classifier__estimator__n_estimators=200, score=0.6417506186792415, total= 8.5min\n",
      "[CV] classifier__estimator__min_samples_split=4, classifier__estimator__n_estimators=200 \n",
      "[CV]  classifier__estimator__min_samples_split=4, classifier__estimator__n_estimators=200, score=0.6402489139368323, total= 8.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 76.8min finished\n"
     ]
    }
   ],
   "source": [
    "# Fitting and predicting RandomForestClassifier with hyperparameters\n",
    "# on improved model\n",
    "parameters_rfc =    {\n",
    "        'classifier__estimator__n_estimators': [100, 200],\n",
    "        'classifier__estimator__min_samples_split': [2, 4]\n",
    "                }\n",
    "\n",
    "model_rfc = Improved_Model(RandomForestClassifier(), parameters=parameters_rfc)\n",
    "model_rfc.fit(x_train[\"message\"], y_train)\n",
    "y_pred = model_rfc.predict(x_test['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Precision  Recall  F1-Score\n",
      "related                      0.81    0.82      0.81\n",
      "request                      0.89    0.89      0.88\n",
      "offer                        0.99    1.00      0.99\n",
      "aid_related                  0.78    0.78      0.78\n",
      "medical_help                 0.91    0.92      0.90\n",
      "medical_products             0.94    0.95      0.93\n",
      "search_and_rescue            0.97    0.97      0.96\n",
      "security                     0.96    0.98      0.97\n",
      "military                     0.95    0.96      0.95\n",
      "water                        0.95    0.96      0.95\n",
      "food                         0.94    0.94      0.94\n",
      "shelter                      0.94    0.94      0.93\n",
      "clothing                     0.98    0.99      0.98\n",
      "money                        0.97    0.98      0.97\n",
      "missing_people               0.98    0.99      0.98\n",
      "refugees                     0.96    0.96      0.94\n",
      "death                        0.95    0.96      0.95\n",
      "other_aid                    0.86    0.87      0.82\n",
      "infrastructure_related       0.86    0.93      0.89\n",
      "transport                    0.95    0.96      0.94\n",
      "buildings                    0.95    0.95      0.93\n",
      "electricity                  0.98    0.98      0.97\n",
      "tools                        0.99    0.99      0.99\n",
      "hospitals                    0.98    0.99      0.98\n",
      "shops                        0.99    0.99      0.99\n",
      "aid_centers                  0.97    0.99      0.98\n",
      "other_infrastructure         0.91    0.95      0.93\n",
      "weather_related              0.89    0.89      0.89\n",
      "floods                       0.96    0.96      0.95\n",
      "storm                        0.94    0.95      0.94\n",
      "fire                         0.98    0.99      0.98\n",
      "earthquake                   0.97    0.97      0.97\n",
      "cold                         0.98    0.98      0.97\n",
      "other_weather                0.95    0.95      0.93\n",
      "direct_report                0.85    0.86      0.83 Precision    0.938000\n",
      "Recall       0.946857\n",
      "F1-Score     0.934000\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# scoring RandomForestClassifier on improved Model\n",
    "scores_rfc, averages_rfc = model_rfc.get_scores(y_test)\n",
    "print(scores_rfc, averages_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__estimator__min_samples_split': 4,\n",
       " 'classifier__estimator__n_estimators': 200}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show best parameters\n",
    "model_rfc.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model\n",
    "pickle.dump(model_rfc, open(model_filepath, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
